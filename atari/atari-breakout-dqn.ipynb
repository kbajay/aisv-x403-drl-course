{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import base64\n",
    "import imageio\n",
    "import IPython\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import pyvirtualdisplay\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.networks import q_network\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.utils import common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.enable_v2_behavior()\n",
    "\n",
    "# Set up a virtual display for rendering OpenAI gym environments.\n",
    "#display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 20000 # @param {type:\"integer\"}\n",
    "\n",
    "initial_collect_steps = 1000  # @param {type:\"integer\"} \n",
    "collect_steps_per_iteration = 1  # @param {type:\"integer\"}\n",
    "replay_buffer_max_length = 100000  # @param {type:\"integer\"}\n",
    "\n",
    "batch_size = 64  # @param {type:\"integer\"}\n",
    "learning_rate = 1e-3  # @param {type:\"number\"}\n",
    "log_interval = 200  # @param {type:\"integer\"}\n",
    "\n",
    "num_eval_episodes = 10  # @param {type:\"integer\"}\n",
    "eval_interval = 1000  # @param {type:\"integer\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.version.VERSION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.envs.make(\"Breakout-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space size: 4\n",
      "['NOOP', 'FIRE', 'RIGHT', 'LEFT']\n",
      "Observation space shape: (210, 160, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Action space size: {}\".format(env.action_space.n))\n",
    "print(env.get_action_meanings()) # env.unwrapped.get_action_meanings() for gym 0.8.0 or later\n",
    "\n",
    "observation = env.reset()\n",
    "print(\"Observation space shape: {}\".format(observation.shape))\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(env.render(mode='rgb_array'))\n",
    "\n",
    "[env.step(2) for x in range(1)]\n",
    "plt.figure()\n",
    "plt.imshow(env.render(mode='rgb_array'))\n",
    "\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'Breakout-v0'\n",
    "env = suite_gym.load(env_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKAAAADSCAIAAABCR1ywAAACu0lEQVR4nO3dsW0TYRiA4QS5RkxARcEIEQNYLpjGEzCBx0AMQGGloEQZBlEgRJEiygL+IZZ9d/bL85Sn090vvfl8v+SzcnMDAMDZ3c55s91u989zttvtbOcfa+rrj+51yjVfnWMxXK7VUjeec1Jfcv6xzjWpUzPBcYtN8LUbfSpc2mSb4DgTfITRdE7xjD8XExy32AQf+1c/9flLXXNqJhjgYt1e43OFl/MMjhM4TuA4geMEjhM4TuA4geMEjhM4TuA4geMEjhM4bvjKzqW9/snfjb72NcFxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdw3PDF94fNZs51cKLvg+MmOE7gOIHjBI4TOG64i35692vOdTARExwncJzAcQLHCRw33EX/fP1nznUwERMcJ3CcwHECxwkcN95Fv3+ccx2c6sfhwyY4TuA4geMEjhM4briL/vz0ds51cKL14LgJjhM4TuA4geMEjhvuoh+/fJpxGZxsffj3hSY4TuA4geMEjhM4briL/ra/m3MdnOjj2j+n/C8JHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdw3Orrm99Lr+E6PGw2R51/t99PtJKDPtzfHzxuguMEjhM4brX0Aq7GzM/UczHBcSY44ko/YAAAAAAAAFjUMyEkRxlHStXAAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=160x210 at 0x7FA0A48614E0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()\n",
    "PIL.Image.fromarray(env.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Spec:\n",
      "BoundedArraySpec(shape=(210, 160, 3), dtype=dtype('uint8'), name='observation', minimum=0, maximum=255)\n"
     ]
    }
   ],
   "source": [
    "print('Observation Spec:')\n",
    "print(env.time_step_spec().observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward Spec:\n",
      "ArraySpec(shape=(), dtype=dtype('float32'), name='reward')\n"
     ]
    }
   ],
   "source": [
    "print('Reward Spec:')\n",
    "print(env.time_step_spec().reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Spec:\n",
      "BoundedArraySpec(shape=(), dtype=dtype('int64'), name='action', minimum=0, maximum=3)\n"
     ]
    }
   ],
   "source": [
    "print('Action Spec:')\n",
    "print(env.action_spec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time step:\n",
      "TimeStep(step_type=array(0, dtype=int32), reward=array(0., dtype=float32), discount=array(1., dtype=float32), observation=array([[[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]],\n",
      "\n",
      "       [[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]],\n",
      "\n",
      "       [[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]],\n",
      "\n",
      "       [[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]],\n",
      "\n",
      "       [[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]]], dtype=uint8))\n",
      "Next time step:\n",
      "TimeStep(step_type=array(1, dtype=int32), reward=array(0., dtype=float32), discount=array(1., dtype=float32), observation=array([[[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]],\n",
      "\n",
      "       [[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]],\n",
      "\n",
      "       [[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]],\n",
      "\n",
      "       [[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]],\n",
      "\n",
      "       [[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]]], dtype=uint8))\n"
     ]
    }
   ],
   "source": [
    "time_step = env.reset()\n",
    "print('Time step:')\n",
    "print(time_step)\n",
    "\n",
    "action = np.array(1, dtype=np.int32)\n",
    "\n",
    "next_time_step = env.step(action)\n",
    "print('Next time step:')\n",
    "print(next_time_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_py_env = suite_gym.load(env_name)\n",
    "eval_py_env = suite_gym.load(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_layer_params = (100,)\n",
    "\n",
    "q_net = q_network.QNetwork(\n",
    "    train_env.observation_spec(),\n",
    "    train_env.action_spec(),\n",
    "    fc_layer_params=fc_layer_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "train_step_counter = tf.Variable(0)\n",
    "\n",
    "agent = dqn_agent.DqnAgent(\n",
    "    train_env.time_step_spec(),\n",
    "    train_env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    optimizer=optimizer,\n",
    "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "    train_step_counter=train_step_counter)\n",
    "\n",
    "agent.initialize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_policy = agent.policy\n",
    "collect_policy = agent.collect_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n",
    "                                                train_env.action_spec())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_environment = tf_py_environment.TFPyEnvironment(\n",
    "    suite_gym.load(env_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_step = example_environment.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([3])>, state=(), info=())"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_policy.action(time_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_return(environment, policy, num_episodes=10):\n",
    "\n",
    "  total_return = 0.0\n",
    "  for _ in range(num_episodes):\n",
    "\n",
    "    time_step = environment.reset()\n",
    "    episode_return = 0.0\n",
    "\n",
    "    while not time_step.is_last():\n",
    "      action_step = policy.action(time_step)\n",
    "      time_step = environment.step(action_step.action)\n",
    "      episode_return += time_step.reward\n",
    "    total_return += episode_return\n",
    "\n",
    "  avg_return = total_return / num_episodes\n",
    "  return avg_return.numpy()[0]\n",
    "\n",
    "\n",
    "# See also the metrics module for standard implementations of different metrics.\n",
    "# https://github.com/tensorflow/agents/tree/master/tf_agents/metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_avg_return(eval_env, random_policy, num_eval_episodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=train_env.batch_size,\n",
    "    max_length=replay_buffer_max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Trajectory(step_type=TensorSpec(shape=(), dtype=tf.int32, name='step_type'), observation=BoundedTensorSpec(shape=(210, 160, 3), dtype=tf.uint8, name='observation', minimum=array(0, dtype=uint8), maximum=array(255, dtype=uint8)), action=BoundedTensorSpec(shape=(), dtype=tf.int64, name='action', minimum=array(0), maximum=array(3)), policy_info=(), next_step_type=TensorSpec(shape=(), dtype=tf.int32, name='step_type'), reward=TensorSpec(shape=(), dtype=tf.float32, name='reward'), discount=BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.collect_data_spec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('step_type',\n",
       " 'observation',\n",
       " 'action',\n",
       " 'policy_info',\n",
       " 'next_step_type',\n",
       " 'reward',\n",
       " 'discount')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.collect_data_spec._fields\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_step(environment, policy, buffer):\n",
    "  time_step = environment.current_time_step()\n",
    "  action_step = policy.action(time_step)\n",
    "  next_time_step = environment.step(action_step.action)\n",
    "  traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "\n",
    "  # Add trajectory to the replay buffer\n",
    "  buffer.add_batch(traj)\n",
    "\n",
    "def collect_data(env, policy, buffer, steps):\n",
    "  for _ in range(steps):\n",
    "    collect_step(env, policy, buffer)\n",
    "\n",
    "collect_data(train_env, random_policy, replay_buffer, steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Trajectory(step_type=<tf.Tensor: shape=(), dtype=int32, numpy=1>, observation=<tf.Tensor: shape=(210, 160, 3), dtype=uint8, numpy=\n",
       " array([[[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]]], dtype=uint8)>, action=<tf.Tensor: shape=(), dtype=int64, numpy=0>, policy_info=(), next_step_type=<tf.Tensor: shape=(), dtype=int32, numpy=1>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(), dtype=float32, numpy=1.0>),\n",
       " BufferInfo(ids=<tf.Tensor: shape=(), dtype=int64, numpy=26>, probabilities=<tf.Tensor: shape=(), dtype=float32, numpy=0.01>))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iter(replay_buffer.as_dataset()).next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: (Trajectory(step_type=(64, 2), observation=(64, 2, 210, 160, 3), action=(64, 2), policy_info=(), next_step_type=(64, 2), reward=(64, 2), discount=(64, 2)), BufferInfo(ids=(64, 2), probabilities=(64,))), types: (Trajectory(step_type=tf.int32, observation=tf.uint8, action=tf.int64, policy_info=(), next_step_type=tf.int32, reward=tf.float32, discount=tf.float32), BufferInfo(ids=tf.int64, probabilities=tf.float32))>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset generates trajectories with shape [Bx2x...]\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3, \n",
    "    sample_batch_size=batch_size, \n",
    "    num_steps=2).prefetch(3)\n",
    "\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.data.ops.iterator_ops.OwnedIterator object at 0x7fa09459af60>\n"
     ]
    }
   ],
   "source": [
    "iterator = iter(dataset)\n",
    "\n",
    "print(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 200: loss = 11965448.0\n",
      "step = 400: loss = 100831608832.0\n",
      "step = 600: loss = 371576930304.0\n",
      "step = 800: loss = 343225630720.0\n",
      "step = 1000: loss = 1101860438016.0\n",
      "step = 1000: Average Return = 0.0\n",
      "step = 1200: loss = 98574171897856.0\n",
      "step = 1400: loss = 153279332352.0\n",
      "step = 1600: loss = 30028812288.0\n",
      "step = 1800: loss = 33531119616.0\n",
      "step = 2000: loss = 114744868864.0\n",
      "step = 2000: Average Return = 0.0\n",
      "step = 2200: loss = 159937036288.0\n",
      "step = 2400: loss = 179160219648.0\n",
      "step = 2600: loss = 30591512576.0\n",
      "step = 2800: loss = 38751719424.0\n",
      "step = 3000: loss = 85041831936.0\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  %%time\n",
    "except:\n",
    "  pass\n",
    "\n",
    "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
    "agent.train = common.function(agent.train)\n",
    "\n",
    "# Reset the train step\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "returns = [avg_return]\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "\n",
    "  # Collect a few steps using collect_policy and save to the replay buffer.\n",
    "  for _ in range(collect_steps_per_iteration):\n",
    "    collect_step(train_env, agent.collect_policy, replay_buffer)\n",
    "\n",
    "  # Sample a batch of data from the buffer and update the agent's network.\n",
    "  experience, unused_info = next(iterator)\n",
    "  train_loss = agent.train(experience).loss\n",
    "\n",
    "  step = agent.train_step_counter.numpy()\n",
    "\n",
    "  if step % log_interval == 0:\n",
    "    print('step = {0}: loss = {1}'.format(step, train_loss))\n",
    "\n",
    "  if step % eval_interval == 0:\n",
    "    avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "    print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
    "    returns.append(avg_return)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_mp4(filename):\n",
    "  \"\"\"Embeds an mp4 file in the notebook.\"\"\"\n",
    "  video = open(filename,'rb').read()\n",
    "  b64 = base64.b64encode(video)\n",
    "  tag = '''\n",
    "  <video width=\"640\" height=\"480\" controls>\n",
    "    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\">\n",
    "  Your browser does not support the video tag.\n",
    "  </video>'''.format(b64.decode())\n",
    "\n",
    "  return IPython.display.HTML(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_policy_eval_video(policy, filename, num_episodes=5, fps=30):\n",
    "  filename = filename + \".mp4\"\n",
    "  with imageio.get_writer(filename, fps=fps) as video:\n",
    "    for _ in range(num_episodes):\n",
    "      time_step = eval_env.reset()\n",
    "      video.append_data(eval_py_env.render())\n",
    "      while not time_step.is_last():\n",
    "        action_step = policy.action(time_step)\n",
    "        time_step = eval_env.step(action_step.action)\n",
    "        video.append_data(eval_py_env.render())\n",
    "  return embed_mp4(filename)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "create_policy_eval_video(agent.policy, \"trained-agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_policy_eval_video(random_policy, \"random-agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
