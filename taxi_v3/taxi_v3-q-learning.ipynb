{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('Taxi-v3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TaxiEnv' object has no attribute 's'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/ajay/code/aisv802_drl_course/taxi_v3/taxi_v3-q-learning.ipynb Cell 2'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/ajay/code/aisv802_drl_course/taxi_v3/taxi_v3-q-learning.ipynb#ch0000002vscode-remote?line=0'>1</a>\u001b[0m env\u001b[39m.\u001b[39;49mrender()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/gym/core.py:286\u001b[0m, in \u001b[0;36mWrapper.render\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/ajay/.local/lib/python3.8/site-packages/gym/core.py?line=284'>285</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrender\u001b[39m(\u001b[39mself\u001b[39m, mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> <a href='file:///home/ajay/.local/lib/python3.8/site-packages/gym/core.py?line=285'>286</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mrender(mode, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/gym/envs/toy_text/taxi.py:222\u001b[0m, in \u001b[0;36mTaxiEnv.render\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    <a href='file:///home/ajay/.local/lib/python3.8/site-packages/gym/envs/toy_text/taxi.py?line=219'>220</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdesc\u001b[39m.\u001b[39mcopy()\u001b[39m.\u001b[39mtolist()\n\u001b[1;32m    <a href='file:///home/ajay/.local/lib/python3.8/site-packages/gym/envs/toy_text/taxi.py?line=220'>221</a>\u001b[0m out \u001b[39m=\u001b[39m [[c\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m line] \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m out]\n\u001b[0;32m--> <a href='file:///home/ajay/.local/lib/python3.8/site-packages/gym/envs/toy_text/taxi.py?line=221'>222</a>\u001b[0m taxi_row, taxi_col, pass_idx, dest_idx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecode(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49ms)\n\u001b[1;32m    <a href='file:///home/ajay/.local/lib/python3.8/site-packages/gym/envs/toy_text/taxi.py?line=223'>224</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mul\u001b[39m(x):\n\u001b[1;32m    <a href='file:///home/ajay/.local/lib/python3.8/site-packages/gym/envs/toy_text/taxi.py?line=224'>225</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m x \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m \u001b[39melse\u001b[39;00m x\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TaxiEnv' object has no attribute 's'"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment\n",
    "\n",
    "- The filled square represents the taxi, which is yellow without a passenger and green with a passenger.\n",
    "- The pipe (\"|\") represents a wall which the taxi cannot cross.\n",
    "- R, G, Y, B are the possible pickup and destination locations. The blue letter represents the current passenger pick-up location, and the purple letter is the current destination.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## States / observation space\n",
    "You'll also notice there are four (4) locations that we can pick up and drop off a passenger: R, G, Y, B or [(0,0), (0,4), (4,0), (4,3)] in (row, col) coordinates. Our illustrated passenger is in location Y and they wish to go to location R.\n",
    "\n",
    "When we also account for one (1) additional passenger state of being inside the taxi, we can take all combinations of passenger locations and destination locations to come to a total number of states for our taxi environment; there's four (4) destinations and five (4 + 1) passenger locations.\n",
    "\n",
    "So, our taxi environment has 5×5×5×4=500 total possible states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action space\n",
    "\n",
    "The agent encounters one of the 500 states and it takes an action. The action in our case can be to move in a direction or decide to pickup/dropoff a passenger.\n",
    "\n",
    "In other words, we have six possible actions:\n",
    "\n",
    "0 = south\n",
    "1 = north\n",
    "2 = east\n",
    "3 = west\n",
    "4 = pickup\n",
    "5 = dropoff\n",
    "\n",
    "This is the action space: the set of all the actions that our agent can take in a given state.\n",
    "\n",
    "You'll notice in the illustration above, that the taxi cannot perform certain actions in certain states due to walls. In environment's code, we will simply provide a -1 penalty for every wall hit and the taxi won't move anywhere. This will just rack up penalties causing the taxi to consider going around the wall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# states:  500\n",
      "# actions:  6\n"
     ]
    }
   ],
   "source": [
    "print('# states: ', env.observation_space.n)\n",
    "print('# actions: ', env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | :\u001b[43m \u001b[0m:\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 0\n",
    "penalties, reward = 0, 0\n",
    "env.reset()\n",
    "\n",
    "frames = [] # for rendering\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action)\n",
    "    \n",
    "    if reward == -10:\n",
    "        penalties += 1\n",
    "        \n",
    "    frames.append({\n",
    "        'frame':env.render(mode='ansi'),\n",
    "        'state':state,\n",
    "        'action':action,\n",
    "        'reward':reward,\n",
    "    })\n",
    "    epochs += 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timesteps taken: 1890\n",
      "Penalties incurred: 616\n"
     ]
    }
   ],
   "source": [
    "print('Timesteps taken: {}'.format(epochs))\n",
    "print('Penalties incurred: {}'.format(penalties))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "def print_frames(frames):\n",
    "    for i, frame in enumerate(frames):\n",
    "        clear_output(wait=True)\n",
    "        print_a_frame(frame, i)\n",
    "        sleep(.5)\n",
    "        \n",
    "def print_a_frame(frame, idx=None):\n",
    "    clear_output(wait=True)\n",
    "    print(frame['frame'])\n",
    "    if idx:\n",
    "        print('Timestep: {}'.format(idx+1))\n",
    "    print('State: {}'.format(frame['state']))\n",
    "    print('Action: {}'.format(frame['action']))\n",
    "    print('Reward: {}'.format(frame['reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "| :\u001b[42m_\u001b[0m| : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "\n",
      "Timestep: 140\n",
      "State: 138\n",
      "Action: 1\n",
      "Reward: -1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-5c6fcc0dfb7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-38a3efb210a8>\u001b[0m in \u001b[0;36mprint_frames\u001b[0;34m(frames)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mclear_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mprint_a_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprint_a_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print_frames(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class QLearning():\n",
    "    def __init__(self, gym_env, epsilon, gamma, alpha):\n",
    "        self.gym_env = gym_env\n",
    "        self._q_table = np.zeros([self.gym_env.observation_space.n, self.gym_env.action_space.n])\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def take_action(self, state):\n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            action = self.gym_env.action_space.sample() # Explore action space\n",
    "        else:\n",
    "            action = np.argmax(self._q_table[state]) # Exploit learned values\n",
    "        return action\n",
    "    \n",
    "    def infer_action(self, state):\n",
    "        action = np.argmax(self._q_table[state])\n",
    "        return action\n",
    "    \n",
    "    def update_q_table(self, state, action, next_state, reward):\n",
    "        old_value = self._q_table[state, action]\n",
    "        next_state_max_q_value = np.max(self._q_table[next_state])\n",
    "        \n",
    "        new_value = (1 - self.alpha) * old_value + self.alpha * (reward + self.gamma * next_state_max_q_value)\n",
    "        self._q_table[state, action] = new_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "epsilon = 0.1\n",
    "num_of_episodes = 100001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_learn = QLearning(env, epsilon, gamma, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 100000\n",
      "Training finished.\n",
      "\n",
      "CPU times: user 1min 53s, sys: 38.6 s, total: 2min 32s\n",
      "Wall time: 1min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\"\"\"Training the agent\"\"\"\n",
    "\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# For plotting metrics\n",
    "all_epochs = []\n",
    "all_penalties = []\n",
    "\n",
    "for i in range(1, num_of_episodes):\n",
    "    state = env.reset()\n",
    "\n",
    "    epochs, penalties, reward, = 0, 0, 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        #get action with epsilon \n",
    "        action = q_learn.take_action(state)\n",
    "        next_state, reward, done, info = env.step(action) \n",
    "        q_learn.update_q_table(state, action, next_state, reward)\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        state = next_state\n",
    "        epochs += 1\n",
    "        \n",
    "    if i % 100 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print('Episode: {}'.format(i))\n",
    "\n",
    "print(\"Training finished.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -2.40417504,  -2.27325184,  -2.4044241 ,  -2.36219966,\n",
       "       -10.51808624, -10.8681522 ])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_learn._q_table[328]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results after 100 num_of_episodes:\n",
      "\tAverage timesteps per episode: 13.18\n",
      "\tAverage penalties per episode: 0.0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Evaluate agent's performance after Q-learning\"\"\"\n",
    "\n",
    "total_epochs, total_penalties = 0, 0\n",
    "num_of_episodes = 100\n",
    "frames = []\n",
    "\n",
    "for _ in range(num_of_episodes):\n",
    "    state = env.reset()\n",
    "    epochs, penalties, reward = 0, 0, 0\n",
    "    \n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        #import pdb; pdb.set_trace()\n",
    "        action = q_learn.infer_action(state)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        frames.append({\n",
    "            'frame':env.render(mode='ansi'),\n",
    "            'state':state,\n",
    "            'action':action,\n",
    "            'reward':reward,\n",
    "        })\n",
    "        \n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        epochs += 1\n",
    "\n",
    "    total_penalties += penalties\n",
    "    total_epochs += epochs\n",
    "\n",
    "print('Results after {} num_of_episodes:'.format(num_of_episodes))\n",
    "print('\\tAverage timesteps per episode: {}'.format(total_epochs / num_of_episodes))\n",
    "print('\\tAverage penalties per episode: {}'.format(total_penalties / num_of_episodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1318"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :\u001b[42mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (Pickup)\n",
      "\n",
      "Timestep: 118\n",
      "State: 96\n",
      "Action: 4\n",
      "Reward: -1\n"
     ]
    }
   ],
   "source": [
    "print_frames(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
